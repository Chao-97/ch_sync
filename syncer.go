package main

import (
	"context"
	"database/sql"
	"errors"
	"fmt"
	"log"
	"strings"
	"time"

	"github.com/shopspring/decimal"
)

// ErrSourceTableEmpty æºè¡¨ä¸ºç©ºé”™è¯¯ï¼ˆç”¨äºè·³è¿‡åŒæ­¥ï¼‰
var ErrSourceTableEmpty = errors.New("source table is empty")

// UniversalSyncer é€šç”¨åŒæ­¥å™¨
type UniversalSyncer struct {
	tableName      string
	tableConfig    TableConfig
	tableSchema    *TableSchema
	sourceDB       *sql.DB
	targetDB       *sql.DB
	config         *Config
	state          *StateManager
	deduplicator   *Deduplicator
	colTypeMap     map[string]string // åˆ—ååˆ°ç±»å‹çš„æ˜ å°„ï¼Œç”¨äºç±»å‹è½¬æ¢
	skipCheckpoint bool              // æ˜¯å¦è·³è¿‡æ–­ç‚¹ç»­ä¼ æ£€æŸ¥ï¼ˆå®æ—¶æ¨¡å¼ä½¿ç”¨ï¼‰
}

// NewUniversalSyncer åˆ›å»ºé€šç”¨åŒæ­¥å™¨
func NewUniversalSyncer(
	tableConfig TableConfig,
	sourceDB, targetDB *sql.DB,
	config *Config,
	state *StateManager,
) (*UniversalSyncer, error) {
	// è‡ªåŠ¨æ£€æµ‹è¡¨ç»“æ„
	schema, err := DetectTableSchema(sourceDB, tableConfig.Name)
	if err != nil {
		return nil, fmt.Errorf("failed to detect schema for %s: %w", tableConfig.Name, err)
	}

	// éªŒè¯æ—¶é—´å­—æ®µæ˜¯å¦å­˜åœ¨
	if !schema.HasColumn(tableConfig.TimeField) {
		return nil, fmt.Errorf("time field '%s' not found in table %s. Available columns: %v",
			tableConfig.TimeField, tableConfig.Name, schema.GetColumnNames())
	}

	// éªŒè¯å»é‡å­—æ®µæ˜¯å¦å­˜åœ¨
	missingKeys := []string{}
	for _, key := range tableConfig.DedupeKeys {
		if !schema.HasColumn(key) {
			missingKeys = append(missingKeys, key)
		}
	}
	if len(missingKeys) > 0 {
		return nil, fmt.Errorf("deduplication keys not found in table %s: %v. Available columns: %v",
			tableConfig.Name, missingKeys, schema.GetColumnNames())
	}

	// åˆ›å»ºå»é‡å™¨
	deduplicator := NewDeduplicator(tableConfig.DedupeKeys, tableConfig.TimeField)

	// æ„å»ºåˆ—ç±»å‹æ˜ å°„
	colTypeMap := make(map[string]string)
	for _, col := range schema.Columns {
		colTypeMap[col.Name] = col.Type
	}

	return &UniversalSyncer{
		tableName:      tableConfig.Name,
		tableConfig:    tableConfig,
		tableSchema:    schema,
		sourceDB:       sourceDB,
		targetDB:       targetDB,
		config:         config,
		state:          state,
		deduplicator:   deduplicator,
		colTypeMap:     colTypeMap,
		skipCheckpoint: false, // é»˜è®¤ä½¿ç”¨æ–­ç‚¹ç»­ä¼ 
	}, nil
}

// Sync æ‰§è¡ŒåŒæ­¥
func (s *UniversalSyncer) Sync(ctx context.Context) error {
	mode := s.tableConfig.GetEffectiveMode(s.config.Sync.Mode)

	if mode == "full" {
		return s.fullSync(ctx)
	}
	return s.incrementalSync(ctx)
}

// SyncWithRealtimeMode æ™ºèƒ½åŒæ­¥ï¼šå…ˆè¿½å¹³å†å²æ•°æ®ï¼Œå†è¿›å…¥å®æ—¶ç›‘æ§æ¨¡å¼
func (s *UniversalSyncer) SyncWithRealtimeMode(ctx context.Context, realtimeThreshold time.Duration) error {
	// 1. æŸ¥è¯¢ç›®æ ‡åº“å’Œæºåº“çš„æœ€æ–°æ—¶é—´
	timeField := s.tableConfig.TimeField
	query := fmt.Sprintf("SELECT MAX(%s) FROM %s", timeField, s.tableName)

	var maxTimeTarget sql.NullTime
	err := s.targetDB.QueryRowContext(ctx, query).Scan(&maxTimeTarget)
	if err != nil && err != sql.ErrNoRows {
		return fmt.Errorf("failed to query target max time: %w", err)
	}

	var maxTimeSource sql.NullTime
	err = s.sourceDB.QueryRowContext(ctx, query).Scan(&maxTimeSource)
	if err != nil && err != sql.ErrNoRows {
		return fmt.Errorf("failed to query source max time: %w", err)
	}

	// éªŒè¯æ—¶é—´æœ‰æ•ˆæ€§ï¼ˆClickHouse æœ‰æ•ˆèŒƒå›´: 1900-01-01 åˆ° 2262-04-11ï¼‰
	minValidTime := time.Date(1900, 1, 1, 0, 0, 0, 0, time.UTC)
	now := time.Now()
	maxFutureTime := now.Add(24 * time.Hour)

	targetTimeValid := maxTimeTarget.Valid && maxTimeTarget.Time.After(minValidTime) && maxTimeTarget.Time.Before(maxFutureTime)
	sourceTimeValid := maxTimeSource.Valid && maxTimeSource.Time.After(minValidTime) && maxTimeSource.Time.Before(maxFutureTime)

	// 2. åˆ¤æ–­æ˜¯å¦éœ€è¦å†å²æ•°æ®è¿½å¹³
	needCatchup := false

	if !targetTimeValid {
		// ç›®æ ‡åº“ä¸ºç©ºæˆ–æ— æ•ˆ
		if !sourceTimeValid {
			// æºåº“ä¹Ÿæ— æ•ˆï¼Œè·³è¿‡
			log.Printf("â­ï¸  %s: æºåº“æ— æ•°æ®ï¼Œè·³è¿‡åŒæ­¥", s.tableName)
			return ErrSourceTableEmpty
		}
		log.Printf("ğŸ“Š %s: ç›®æ ‡åº“ä¸ºç©ºæˆ–æ—¶é—´æ— æ•ˆï¼Œå¼€å§‹åˆå§‹åŒ–åŒæ­¥...", s.tableName)
		needCatchup = true
	} else if sourceTimeValid {
		// éƒ½æœ‰æ•ˆï¼Œè®¡ç®—å»¶è¿Ÿï¼ˆç”¨æºåº“å’Œç›®æ ‡åº“çš„å·®å€¼ï¼‰
		lag := maxTimeSource.Time.Sub(maxTimeTarget.Time)
		if lag > realtimeThreshold {
			log.Printf("ğŸ“Š %s: æ•°æ®å»¶è¿Ÿ %sï¼ˆæºåº“: %s, ç›®æ ‡åº“: %sï¼‰ï¼Œå¼€å§‹è¿½å¹³å†å²æ•°æ®...",
				s.tableName, FormatDuration(lag),
				maxTimeSource.Time.Format("2006-01-02 15:04:05"),
				maxTimeTarget.Time.Format("2006-01-02 15:04:05"))
			needCatchup = true
		}
	}

	if needCatchup {
		// å†å²è¿½å¹³æ¨¡å¼ï¼šä½¿ç”¨æ–­ç‚¹ç»­ä¼ 
		s.skipCheckpoint = false

		// æ‰§è¡Œå†å²æ•°æ®åŒæ­¥
		if err := s.incrementalSync(ctx); err != nil {
			// å¦‚æœæ˜¯æºè¡¨ä¸ºç©ºé”™è¯¯ï¼Œç›´æ¥è¿”å›
			if errors.Is(err, ErrSourceTableEmpty) {
				return err
			}
			return fmt.Errorf("failed to catch up historical data: %w", err)
		}

		log.Printf("âœ… %s: å†å²æ•°æ®å·²è¿½å¹³", s.tableName)
	}

	// 3. è¿›å…¥å®æ—¶å¢é‡æ¨¡å¼ï¼šä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ 
	log.Printf("ğŸ”„ %s: å·²è¿›å…¥å®æ—¶å¢é‡æ¨¡å¼ï¼ˆç›‘æ§æœ€æ–°å˜åŒ–ï¼‰", s.tableName)
	s.skipCheckpoint = true
	return s.realtimeIncrementalSync(ctx)
}

// realtimeIncrementalSync å®æ—¶å¢é‡åŒæ­¥ï¼ˆåªåŒæ­¥æœ€æ–°çš„æ—¶é—´çª—å£ï¼‰
// ä½¿ç”¨åŒå‘æ—¶é—´çª—å£æ£€æŸ¥ï¼Œé˜²æ­¢æ•°æ®åº“åˆ‡æ¢æ—¶çš„æ•°æ®ä¸¢å¤±
func (s *UniversalSyncer) realtimeIncrementalSync(ctx context.Context) error {
	timeField := s.tableConfig.TimeField
	query := fmt.Sprintf("SELECT MAX(%s) FROM %s", timeField, s.tableName)

	// 1. æŸ¥è¯¢ç›®æ ‡åº“æœ€æ–°æ—¶é—´
	var maxTimeTarget sql.NullTime
	err := s.targetDB.QueryRowContext(ctx, query).Scan(&maxTimeTarget)
	if err != nil && err != sql.ErrNoRows {
		return fmt.Errorf("failed to query target max time: %w", err)
	}

	// 2. æŸ¥è¯¢æºåº“æœ€æ–°æ—¶é—´
	var maxTimeSource sql.NullTime
	err = s.sourceDB.QueryRowContext(ctx, query).Scan(&maxTimeSource)
	if err != nil && err != sql.ErrNoRows {
		return fmt.Errorf("failed to query source max time: %w", err)
	}

	// éªŒè¯æ—¶é—´æœ‰æ•ˆæ€§
	minValidTime := time.Date(1900, 1, 1, 0, 0, 0, 0, time.UTC)
	now := time.Now()
	maxFutureTime := now.Add(24 * time.Hour)

	targetTimeValid := maxTimeTarget.Valid && maxTimeTarget.Time.After(minValidTime) && maxTimeTarget.Time.Before(maxFutureTime)
	sourceTimeValid := maxTimeSource.Valid && maxTimeSource.Time.After(minValidTime) && maxTimeSource.Time.Before(maxFutureTime)

	// 3. ç¡®å®šåŒæ­¥æ—¶é—´çª—å£
	var startTime, endTime time.Time
	backwardWindow := 5 * time.Minute // å›æº¯çª—å£

	if !targetTimeValid {
		// ç›®æ ‡åº“ä¸ºç©ºæˆ–æ—¶é—´æ— æ•ˆ
		if !sourceTimeValid {
			// æºåº“ä¹Ÿæ— æœ‰æ•ˆæ•°æ®ï¼Œä¸åŒæ­¥
			return nil
		}
		// ä»5åˆ†é’Ÿå‰å¼€å§‹
		startTime = now.Add(-backwardWindow)
		endTime = maxTimeSource.Time
	} else if !sourceTimeValid {
		// æºåº“ä¸ºç©ºï¼ˆç½•è§æƒ…å†µï¼‰ï¼Œä¸åŒæ­¥
		return nil
	} else {
		// 4. åŒå‘æ—¶é—´çª—å£ç­–ç•¥
		// ä½¿ç”¨å›æº¯çª—å£ä»ç›®æ ‡åº“æœ€æ–°æ—¶é—´å¾€å‰æ£€æŸ¥
		startTime = maxTimeTarget.Time.Add(-backwardWindow)
		// endTime ä½¿ç”¨æºåº“æœ€å¤§æ—¶é—´ï¼Œå¹¶åŠ  1 ç§’ç¡®ä¿åŒ…å«è¾¹ç•Œæ•°æ®
		endTime = maxTimeSource.Time.Add(1 * time.Second)

		// 5. æ£€æµ‹æ•°æ®åº“åˆ‡æ¢åœºæ™¯
		if maxTimeSource.Time.Before(maxTimeTarget.Time) {
			log.Printf("âš ï¸  %s: æ£€æµ‹åˆ°æºåº“æ—¶é—´(%s)æ—©äºç›®æ ‡åº“æ—¶é—´(%s)ï¼Œå¯èƒ½å‘ç”Ÿäº†æ•°æ®åº“åˆ‡æ¢",
				s.tableName,
				maxTimeSource.Time.Format("2006-01-02 15:04:05"),
				maxTimeTarget.Time.Format("2006-01-02 15:04:05"))
			log.Printf("ğŸ” %s: å›æº¯æ£€æŸ¥æœ€è¿‘ %v çš„æ•°æ®ï¼Œç¡®ä¿ä¸é—æ¼åˆ‡æ¢çª—å£æœŸçš„æ•°æ®...",
				s.tableName, backwardWindow)

			// åœ¨åˆ‡æ¢åœºæ™¯ä¸‹ï¼ŒendTime ä½¿ç”¨æºåº“æœ€å¤§æ—¶é—´ + 1ç§’
			// startTime å·²ç»æ˜¯ maxTimeTarget - backwardWindow
			// è¿™æ ·å¯ä»¥æ•è·åˆ‡æ¢çª—å£æœŸå†…æœªåŒæ­¥çš„æ•°æ®
		} else {
			// æ­£å¸¸åœºæ™¯ï¼šæºåº“æ—¶é—´ >= ç›®æ ‡åº“æ—¶é—´
			// ä½¿ç”¨è¾ƒå°çš„å›æº¯çª—å£ï¼ˆ5ç§’ï¼‰ï¼Œæé«˜å®æ—¶æ€§
			startTime = maxTimeTarget.Time.Add(-5 * time.Second)
		}
	}

	// 2. æŸ¥è¯¢æºåº“æ˜¯å¦æœ‰æ–°æ•°æ®
	countQuery := fmt.Sprintf("SELECT COUNT(*) FROM %s WHERE %s >= ? AND %s <= ?",
		s.tableName, timeField, timeField)

	var newRecordCount int64
	err = s.sourceDB.QueryRowContext(ctx, countQuery, startTime, endTime).Scan(&newRecordCount)
	if err != nil {
		return fmt.Errorf("failed to count new records: %w", err)
	}

	if newRecordCount == 0 {
		// æ²¡æœ‰æ–°æ•°æ®ï¼Œé™é»˜è¿”å›
		return nil
	}

	log.Printf("ğŸ” %s: æ£€æµ‹åˆ° %d æ¡æ–°è®°å½•ï¼ˆ%s ~ %sï¼‰",
		s.tableName, newRecordCount,
		startTime.Format("15:04:05"),
		endTime.Format("15:04:05"))

	// 3. åŒæ­¥æ–°æ•°æ®
	segment := TimeSegment{Start: startTime, End: endTime}
	recordCount, err := s.syncSegment(ctx, segment)
	if err != nil {
		return fmt.Errorf("failed to sync new records: %w", err)
	}

	if recordCount > 0 {
		log.Printf("âœ… %s: å®æ—¶åŒæ­¥å®Œæˆï¼Œæ–°å¢ %d æ¡è®°å½•", s.tableName, recordCount)
	}

	return nil
}

// incrementalSync å¢é‡åŒæ­¥
func (s *UniversalSyncer) incrementalSync(ctx context.Context) error {
	// 1. ç¡®å®šæ—¶é—´èŒƒå›´
	timeRange, err := s.determineTimeRange()
	if err != nil {
		return err
	}

	// å¦‚æœæ—¶é—´èŒƒå›´æ— æ•ˆï¼ˆå¼€å§‹æ—¶é—´>=ç»“æŸæ—¶é—´ï¼‰ï¼Œè·³è¿‡åŒæ­¥
	if !timeRange.Start.Before(timeRange.End) {
		log.Printf("â­ï¸  %s: æ— éœ€åŒæ­¥ï¼ˆå·²æ˜¯æœ€æ–°ï¼‰", s.tableName)
		return nil
	}

	log.Printf("ğŸ“Š %s: åŒæ­¥æ—¶é—´èŒƒå›´ %s ~ %s",
		s.tableName, timeRange.Start.Format(time.RFC3339), timeRange.End.Format(time.RFC3339))

	// 2. æŒ‰å¤©åˆ†æ®µ
	segments := s.segmentTimeRange(timeRange)
	log.Printf("ğŸ“¦ %s: åˆ†ä¸º %d ä¸ªæ—¥åˆ†æ®µ", s.tableName, len(segments))

	// 3. é€æ®µåŒæ­¥
	totalRecords := 0
	for i, segment := range segments {
		// æ£€æŸ¥æ˜¯å¦å·²å®Œæˆï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
		if !s.skipCheckpoint && s.state.IsSegmentCompleted(s.tableName, segment) {
			log.Printf("â­ï¸  %s: åˆ†æ®µ %d/%d å·²å®Œæˆï¼Œè·³è¿‡", s.tableName, i+1, len(segments))
			continue
		}

		// åŒæ­¥è¯¥åˆ†æ®µ
		recordCount, err := s.syncSegment(ctx, segment)
		if err != nil {
			return fmt.Errorf("failed to sync segment %v: %w", segment, err)
		}

		totalRecords += recordCount

		// ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä»…åœ¨éè·³è¿‡æ£€æŸ¥ç‚¹æ¨¡å¼ä¸‹ï¼‰
		if !s.skipCheckpoint {
			s.state.MarkSegmentCompleted(s.tableName, segment, recordCount)
		}

		log.Printf("âœ… %s: åˆ†æ®µ %d/%d å®Œæˆï¼ŒåŒæ­¥ %d æ¡è®°å½•",
			s.tableName, i+1, len(segments), recordCount)
	}

	log.Printf("ğŸ‰ %s: å¢é‡åŒæ­¥å®Œæˆï¼Œæ€»è®¡ %d æ¡è®°å½•", s.tableName, totalRecords)
	return nil
}

// determineTimeRange ç¡®å®šåŒæ­¥çš„æ—¶é—´èŒƒå›´
func (s *UniversalSyncer) determineTimeRange() (TimeRange, error) {
	timeField := s.tableConfig.TimeField

	var startTime, endTime time.Time

	log.Printf("â±ï¸  %s: å¼€å§‹ç¡®å®šæ—¶é—´èŒƒå›´...", s.tableName)

	// ç¡®å®šç»“æŸæ—¶é—´
	if s.config.TimeRange.End != "" {
		var err error
		endTime, err = time.Parse(time.RFC3339, s.config.TimeRange.End)
		if err != nil {
			return TimeRange{}, fmt.Errorf("invalid end time: %w", err)
		}
		log.Printf("â±ï¸  %s: ä½¿ç”¨é…ç½®çš„ç»“æŸæ—¶é—´: %s", s.tableName, endTime.Format(time.RFC3339))
	} else {
		// æŸ¥è¯¢æºåº“çš„æœ€æ–°æ—¶é—´ä½œä¸ºç»“æŸæ—¶é—´
		query := fmt.Sprintf("SELECT MAX(%s) FROM %s", timeField, s.tableName)
		ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
		defer cancel()

		var maxTimeSource sql.NullTime
		err := s.sourceDB.QueryRowContext(ctx, query).Scan(&maxTimeSource)
		if err != nil && err != sql.ErrNoRows {
			return TimeRange{}, fmt.Errorf("failed to query source max time: %w", err)
		}

		// éªŒè¯æºåº“æ—¶é—´æœ‰æ•ˆæ€§
		minValidTime := time.Date(1900, 1, 1, 0, 0, 0, 0, time.UTC)
		now := time.Now()
		maxFutureTime := now.Add(24 * time.Hour)

		if maxTimeSource.Valid && maxTimeSource.Time.After(minValidTime) && maxTimeSource.Time.Before(maxFutureTime) {
			// ä½¿ç”¨æºåº“æœ€æ–°æ—¶é—´ + 1ç§’ï¼Œç¡®ä¿åŒ…å«è¾¹ç•Œæ•°æ®
			endTime = maxTimeSource.Time.Add(1 * time.Second)
			log.Printf("â±ï¸  %s: ä½¿ç”¨æºåº“æœ€æ–°æ—¶é—´ä½œä¸ºç»“æŸæ—¶é—´: %s (å«è¾¹ç•Œ)", s.tableName, maxTimeSource.Time.Format(time.RFC3339))
		} else {
			// æºåº“æ— æœ‰æ•ˆæ•°æ®
			log.Printf("â­ï¸  %s: æºåº“æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡åŒæ­¥", s.tableName)
			return TimeRange{}, ErrSourceTableEmpty
		}
	}

	// ç¡®å®šå¼€å§‹æ—¶é—´
	if s.config.TimeRange.AutoDetect {
		// æŸ¥è¯¢ç›®æ ‡åº“çš„æœ€å¤§æ—¶é—´
		log.Printf("ğŸ” %s: æ­£åœ¨æŸ¥è¯¢ç›®æ ‡åº“æœ€æ–°æ—¶é—´ï¼ˆå­—æ®µ: %sï¼‰...", s.tableName, timeField)
		query := fmt.Sprintf("SELECT MAX(%s) FROM %s", timeField, s.tableName)

		ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
		defer cancel()

		var maxTime sql.NullTime
		err := s.targetDB.QueryRowContext(ctx, query).Scan(&maxTime)
		if err != nil && err != sql.ErrNoRows {
			log.Printf("âŒ %s: æŸ¥è¯¢æœ€å¤§æ—¶é—´å¤±è´¥: %v", s.tableName, err)
			return TimeRange{}, fmt.Errorf("failed to query max time: %w", err)
		}

		// éªŒè¯æ—¶é—´æœ‰æ•ˆæ€§ï¼ˆClickHouse æœ‰æ•ˆèŒƒå›´: 1900-01-01 åˆ° 2262-04-11ï¼‰
		minValidTime := time.Date(1900, 1, 1, 0, 0, 0, 0, time.UTC)
		isValidTime := maxTime.Valid && maxTime.Time.After(minValidTime) && maxTime.Time.Before(endTime.Add(24*time.Hour))

		if isValidTime {
			startTime = maxTime.Time.Add(1 * time.Millisecond) // ä»æœ€å¤§æ—¶é—´å 1ms å¼€å§‹
			log.Printf("ğŸ” %s: æ£€æµ‹åˆ°ç›®æ ‡åº“æœ€æ–°æ—¶é—´ %sï¼Œä»è¯¥æ—¶é—´åå¼€å§‹åŒæ­¥", s.tableName, maxTime.Time.Format(time.RFC3339))
		} else {
			// ç›®æ ‡åº“ä¸ºç©ºï¼Œæ£€æŸ¥æºåº“æ˜¯å¦æœ‰æ•°æ®
			log.Printf("ğŸ” %s: ç›®æ ‡åº“ä¸ºç©ºï¼Œæ£€æŸ¥æºåº“æ˜¯å¦æœ‰æ•°æ®...", s.tableName)
			sourceQuery := fmt.Sprintf("SELECT MIN(%s), MAX(%s) FROM %s", timeField, timeField, s.tableName)

			var minTimeSource, maxTimeSource sql.NullTime
			err := s.sourceDB.QueryRowContext(ctx, sourceQuery).Scan(&minTimeSource, &maxTimeSource)
			if err != nil && err != sql.ErrNoRows {
				log.Printf("âŒ %s: æŸ¥è¯¢æºåº“æ—¶é—´èŒƒå›´å¤±è´¥: %v", s.tableName, err)
				return TimeRange{}, fmt.Errorf("failed to query source time range: %w", err)
			}

			// å¦‚æœæºåº“ä¹Ÿæ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡åŒæ­¥
			if !minTimeSource.Valid || !maxTimeSource.Valid {
				log.Printf("â­ï¸  %s: æºåº“æ— æ•°æ®ï¼Œè·³è¿‡åŒæ­¥", s.tableName)
				return TimeRange{}, ErrSourceTableEmpty
			}

			// æºåº“æœ‰æ•°æ®ï¼Œä½¿ç”¨ fallback æ—¶é—´æˆ–æºåº“æœ€å°æ—¶é—´
			fallbackTime := time.Now().AddDate(0, 0, -s.config.TimeRange.FallbackDays)
			if minTimeSource.Time.After(fallbackTime) {
				// å¦‚æœæºåº“æœ€æ—©æ•°æ®æ¯” fallback æ—¶é—´è¿˜æ–°ï¼Œå°±ä»æºåº“æœ€æ—©æ•°æ®å¼€å§‹
				startTime = minTimeSource.Time
				log.Printf("ğŸ” %s: æºåº“æœ€æ—©æ•°æ®æ—¶é—´ %sï¼Œä»è¯¥æ—¶é—´å¼€å§‹åŒæ­¥", s.tableName, minTimeSource.Time.Format(time.RFC3339))
			} else {
				// å¦åˆ™ä½¿ç”¨ fallback æ—¶é—´
				startTime = fallbackTime
				log.Printf("âš ï¸  %s: ç›®æ ‡åº“ä¸ºç©ºï¼Œä» %d å¤©å‰å¼€å§‹: %s", s.tableName, s.config.TimeRange.FallbackDays, startTime.Format(time.RFC3339))
			}
		}
	} else if s.config.TimeRange.Start != "" {
		var err error
		startTime, err = time.Parse(time.RFC3339, s.config.TimeRange.Start)
		if err != nil {
			return TimeRange{}, fmt.Errorf("invalid start time: %w", err)
		}
		log.Printf("â±ï¸  %s: ä½¿ç”¨é…ç½®çš„å¼€å§‹æ—¶é—´: %s", s.tableName, startTime.Format(time.RFC3339))
	} else {
		startTime = time.Now().AddDate(0, 0, -30) // é»˜è®¤ 30 å¤©
		log.Printf("â±ï¸  %s: ä½¿ç”¨é»˜è®¤30å¤©å‰ä½œä¸ºå¼€å§‹æ—¶é—´: %s", s.tableName, startTime.Format(time.RFC3339))
	}

	log.Printf("âœ… %s: æ—¶é—´èŒƒå›´ç¡®å®šå®Œæˆ", s.tableName)
	return TimeRange{Start: startTime, End: endTime}, nil
}

// syncSegment åŒæ­¥ä¸€ä¸ªæ—¶é—´åˆ†æ®µ
func (s *UniversalSyncer) syncSegment(ctx context.Context, segment TimeSegment) (int, error) {
	timeField := s.tableConfig.TimeField
	batchSize := s.tableConfig.GetEffectiveBatchSize(s.config.Sync.BatchSize)

	log.Printf("â° %s: åŒæ­¥æ—¶é—´æ®µ %s ~ %s",
		s.tableName,
		segment.Start.Format("2006-01-02 15:04:05"),
		segment.End.Format("2006-01-02 15:04:05"))

	// 1. æŸ¥è¯¢ç›®æ ‡åº“å·²å­˜åœ¨çš„å»é‡é”®
	existingKeys, err := s.deduplicator.FetchExistingKeys(
		s.targetDB, s.tableName, segment, s.tableSchema,
	)
	if err != nil {
		return 0, fmt.Errorf("failed to fetch existing keys: %w", err)
	}
	log.Printf("ğŸ”‘ %s: ç›®æ ‡åº“å·²æœ‰ %d æ¡è®°å½•ï¼ˆè¯¥æ—¶é—´æ®µï¼‰", s.tableName, len(existingKeys))

	// 2. æ„å»ºæŸ¥è¯¢ SQLï¼ˆæŸ¥è¯¢æ‰€æœ‰å­—æ®µï¼‰
	columns := s.tableSchema.GetColumnNames()
	columnsStr := strings.Join(columns, ", ")

	query := fmt.Sprintf(
		"SELECT %s FROM %s WHERE %s >= ? AND %s < ? ORDER BY %s",
		columnsStr, s.tableName, timeField, timeField, timeField,
	)

	// 3. æµå¼æŸ¥è¯¢æºåº“æ•°æ®
	log.Printf("ğŸ” %s: å¼€å§‹æŸ¥è¯¢æºåº“æ•°æ®...", s.tableName)
	rows, err := s.sourceDB.QueryContext(ctx, query, segment.Start, segment.End)
	if err != nil {
		return 0, fmt.Errorf("failed to query source: %w", err)
	}
	defer rows.Close()

	// 4. æ‰¹é‡è¯»å–ã€å»é‡ã€æ’å…¥
	totalInserted := 0
	totalScanned := 0
	totalSkipped := 0
	batch := make([]map[string]interface{}, 0, batchSize)
	batchCount := 0

	for rows.Next() {
		totalScanned++

		// æ‰«æä¸€è¡Œæ•°æ®
		record, err := s.scanRow(rows, columns)
		if err != nil {
			return totalInserted, fmt.Errorf("failed to scan row: %w", err)
		}

		// æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå»é‡ï¼‰
		key := s.deduplicator.BuildKey(record)
		if existingKeys[key] {
			totalSkipped++
			continue // è·³è¿‡å·²å­˜åœ¨çš„è®°å½•
		}

		batch = append(batch, record)

		// æ‰¹é‡æ’å…¥
		if len(batch) >= batchSize {
			batchCount++
			inserted, err := s.insertBatch(ctx, batch, columns)
			if err != nil {
				return totalInserted, fmt.Errorf("failed to insert batch: %w", err)
			}
			totalInserted += inserted
			batch = batch[:0] // æ¸…ç©º

			log.Printf("ğŸ“¦ %s: æ‰¹æ¬¡ #%d æ’å…¥ %d æ¡ | ç´¯è®¡: æ‰«æ %d, æ’å…¥ %d, è·³è¿‡ %d",
				s.tableName, batchCount, inserted, totalScanned, totalInserted, totalSkipped)
		}
	}

	if err := rows.Err(); err != nil {
		return totalInserted, fmt.Errorf("error iterating rows: %w", err)
	}

	// 5. æ’å…¥å‰©ä½™æ•°æ®
	if len(batch) > 0 {
		batchCount++
		inserted, err := s.insertBatch(ctx, batch, columns)
		if err != nil {
			return totalInserted, fmt.Errorf("failed to insert final batch: %w", err)
		}
		totalInserted += inserted

		log.Printf("ğŸ“¦ %s: æ‰¹æ¬¡ #%d æ’å…¥ %d æ¡ | ç´¯è®¡: æ‰«æ %d, æ’å…¥ %d, è·³è¿‡ %d",
			s.tableName, batchCount, inserted, totalScanned, totalInserted, totalSkipped)
	}

	log.Printf("âœ¨ %s: æ—¶é—´æ®µå®Œæˆ - æ‰«æ %d æ¡, æ–°å¢ %d æ¡, è·³è¿‡ %d æ¡",
		s.tableName, totalScanned, totalInserted, totalSkipped)

	return totalInserted, nil
}

// scanRow æ‰«æä¸€è¡Œæ•°æ®åˆ° map
func (s *UniversalSyncer) scanRow(rows *sql.Rows, columns []string) (map[string]interface{}, error) {
	values := make([]interface{}, len(columns))
	valuePtrs := make([]interface{}, len(columns))
	for i := range values {
		valuePtrs[i] = &values[i]
	}

	if err := rows.Scan(valuePtrs...); err != nil {
		return nil, err
	}

	record := make(map[string]interface{})
	for i, col := range columns {
		record[col] = values[i]
	}

	return record, nil
}

// insertBatch æ‰¹é‡æ’å…¥æ•°æ®
func (s *UniversalSyncer) insertBatch(ctx context.Context, batch []map[string]interface{}, columns []string) (int, error) {
	if len(batch) == 0 {
		return 0, nil
	}

	// ä½¿ç”¨ ClickHouse åŸç”Ÿæ‰¹é‡æ’å…¥
	columnsStr := strings.Join(columns, ", ")
	query := fmt.Sprintf("INSERT INTO %s (%s)", s.tableName, columnsStr)

	// å¼€å§‹æ‰¹é‡æ’å…¥
	tx, err := s.targetDB.Begin()
	if err != nil {
		return 0, fmt.Errorf("failed to begin transaction: %w", err)
	}
	defer tx.Rollback()

	stmt, err := tx.PrepareContext(ctx, query)
	if err != nil {
		return 0, fmt.Errorf("failed to prepare statement: %w", err)
	}
	defer stmt.Close()

	// é€è¡Œæ’å…¥
	for _, record := range batch {
		values := make([]interface{}, len(columns))
		for i, col := range columns {
			val := record[col]

			// ç‰¹æ®Šå¤„ç† Decimal ç±»å‹ï¼šå°† string è½¬ä¸º decimal.Decimal
			if typeStr, ok := s.colTypeMap[col]; ok && strings.Contains(typeStr, "Decimal") {
				if valStr, ok := val.(string); ok {
					if d, err := decimal.NewFromString(valStr); err == nil {
						values[i] = d
						continue
					}
				} else if valBytes, ok := val.([]byte); ok {
					// æŸäº›é©±åŠ¨å¯èƒ½è¿”å› []byte
					if d, err := decimal.NewFromString(string(valBytes)); err == nil {
						values[i] = d
						continue
					}
				}
			}

			// ç‰¹æ®Šå¤„ç† DateTime ç±»å‹ï¼šéªŒè¯æ—¶é—´èŒƒå›´
			if typeStr, ok := s.colTypeMap[col]; ok && strings.Contains(typeStr, "DateTime") {
				if t, ok := val.(time.Time); ok {
					// ClickHouse DateTime èŒƒå›´: 1900-01-01 åˆ° 2262-04-11
					minTime := time.Date(1900, 1, 1, 0, 0, 0, 0, time.UTC)
					maxTime := time.Date(2262, 4, 11, 23, 47, 16, 0, time.UTC)

					if t.Before(minTime) || t.After(maxTime) || t.IsZero() {
						// è¶…å‡ºèŒƒå›´æˆ–é›¶å€¼ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´ï¼ˆ1970-01-01ï¼‰
						values[i] = time.Date(1970, 1, 1, 0, 0, 0, 0, time.UTC)
						continue
					}
				}
			}

			values[i] = val
		}

		_, err := stmt.ExecContext(ctx, values...)
		if err != nil {
			return 0, fmt.Errorf("failed to insert row: %w", err)
		}
	}

	// æäº¤äº‹åŠ¡
	if err := tx.Commit(); err != nil {
		return 0, fmt.Errorf("failed to commit transaction: %w", err)
	}

	return len(batch), nil
}

// fullSync å…¨é‡åŒæ­¥
func (s *UniversalSyncer) fullSync(ctx context.Context) error {
	log.Printf("ğŸ”„ %s: å¼€å§‹å…¨é‡åŒæ­¥", s.tableName)

	batchSize := s.tableConfig.GetEffectiveBatchSize(s.config.Sync.BatchSize)
	columns := s.tableSchema.GetColumnNames()
	columnsStr := strings.Join(columns, ", ")

	query := fmt.Sprintf("SELECT %s FROM %s", columnsStr, s.tableName)

	rows, err := s.sourceDB.QueryContext(ctx, query)
	if err != nil {
		return fmt.Errorf("failed to query source: %w", err)
	}
	defer rows.Close()

	totalInserted := 0
	batch := make([]map[string]interface{}, 0, batchSize)

	for rows.Next() {
		record, err := s.scanRow(rows, columns)
		if err != nil {
			return fmt.Errorf("failed to scan row: %w", err)
		}

		batch = append(batch, record)

		if len(batch) >= batchSize {
			inserted, err := s.insertBatch(ctx, batch, columns)
			if err != nil {
				return fmt.Errorf("failed to insert batch: %w", err)
			}
			totalInserted += inserted
			batch = batch[:0]

			log.Printf("ğŸ“¦ %s: å·²åŒæ­¥ %d æ¡è®°å½•", s.tableName, totalInserted)
		}
	}

	if err := rows.Err(); err != nil {
		return fmt.Errorf("error iterating rows: %w", err)
	}

	if len(batch) > 0 {
		inserted, err := s.insertBatch(ctx, batch, columns)
		if err != nil {
			return fmt.Errorf("failed to insert final batch: %w", err)
		}
		totalInserted += inserted
	}

	log.Printf("ğŸ‰ %s: å…¨é‡åŒæ­¥å®Œæˆï¼Œæ€»è®¡ %d æ¡è®°å½•", s.tableName, totalInserted)
	return nil
}

// segmentTimeRange å°†æ—¶é—´èŒƒå›´åˆ†å‰²ä¸ºæŒ‰å¤©çš„åˆ†æ®µ
func (s *UniversalSyncer) segmentTimeRange(timeRange TimeRange) []TimeSegment {
	if !s.config.Sync.DailySegmentation {
		return []TimeSegment{{Start: timeRange.Start, End: timeRange.End}}
	}

	segments := []TimeSegment{}
	current := timeRange.Start

	for current.Before(timeRange.End) {
		dayEnd := time.Date(current.Year(), current.Month(), current.Day(), 23, 59, 59, 999999999, current.Location())
		dayEnd = dayEnd.Add(1 * time.Nanosecond) // ä¸‹ä¸€å¤©çš„ 00:00:00

		if dayEnd.After(timeRange.End) {
			dayEnd = timeRange.End
		}

		segments = append(segments, TimeSegment{Start: current, End: dayEnd})
		current = dayEnd
	}

	return segments
}
